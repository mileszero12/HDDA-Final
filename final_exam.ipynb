{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desirable-crest",
   "metadata": {},
   "source": [
    "# **Final Exam** \n",
    "### *Tan Yu*\n",
    "***\n",
    "`Here are some functions:`\n",
    "####  1.  *Import and preprocess the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liquid-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas\n",
    "import numpy as np\n",
    "import operator\n",
    "xtest = pandas.read_csv('bbb.test.csv', header = None)\n",
    "xtrain = pandas.read_csv('bbb.train.csv',header = None)\n",
    "ytest = pandas.read_csv('labels.test.csv',header = None)\n",
    "ytrain = pandas.read_csv('labels.train.csv',header = None)\n",
    "yytrain = ytrain.replace('nc', 1)\n",
    "fytrain = yytrain.replace('c', 0)\n",
    "yytest = ytest.replace('nc', 1)\n",
    "fytest = yytest.replace('c', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-berry",
   "metadata": {},
   "source": [
    "#### 2. *The Confusion Matrix Generator Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "honest-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionM(truey, prey):\n",
    "    v00, v01, v10, v11 = 0, 0, 0, 0\n",
    "    for i in range(len(truey)):\n",
    "        if truey[i] == 0:\n",
    "            if prey[i] == 0:\n",
    "                v00 += 1\n",
    "            else:\n",
    "                v10 += 1\n",
    "        else:\n",
    "            if prey[i] == 0:\n",
    "                v01 += 1\n",
    "            else:\n",
    "                v11 += 1\n",
    "    print('Confusion Matrix|', 'True c(0)', 'True nc(1)', 'Total')\n",
    "    print('  Predict c(0)  |',  v00,'       ', v01,'       ', v00+v01)\n",
    "    print('  Predict nc(1) |',  v10,'      ', v11,'       ', v10+v11)\n",
    "    print('  Total         |',  v10+v00,'      ', v11+v01,'       ', v10+v11+v00+v01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-springfield",
   "metadata": {},
   "source": [
    "#### 3.*The prediction function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "double-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictFunc(x, y, beta):\n",
    "    res = {}\n",
    "    res['error'] = 0\n",
    "    res['prey'] = []\n",
    "    res['truey'] = []\n",
    "    for i in range(x.shape[0]):\n",
    "        temp = x.iloc[i,:]\n",
    "        tempty = y.iloc[i,0]\n",
    "        temppy = PredOneRow(temp, beta)\n",
    "        res['prey'].append(temppy)\n",
    "        res['truey'].append(tempty)\n",
    "        if tempty != temppy:  \n",
    "            res['error'] += 1\n",
    "    res['accu'] = 1 - res['error'] / x.shape[0]\n",
    "    res['mse'] = res['error'] / x.shape[0]\n",
    "    return res\n",
    "\n",
    "def PredOneRow(x, beta):\n",
    "    x = np.hstack((np.ones((1)), x.T))\n",
    "    pre = pi_val(np.dot(x, beta))\n",
    "    if pre > 0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-showcase",
   "metadata": {},
   "source": [
    "#### 4.*The function to obtain $\\pi_i$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improved-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_val(input):\n",
    "    output = 1 / (1 + np.exp(-input))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-guarantee",
   "metadata": {},
   "source": [
    "***\n",
    "`Here are some alternative denotations:`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-chest",
   "metadata": {},
   "source": [
    "$\\beta = (\\beta_0, \\beta_1, ...,\\beta_p)^T \\\\$ \n",
    "$x_i = (1, x_{i1}, ..., x_{ip})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-broadway",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem (a) : *Log-likelihood function*\n",
    "\\begin{align*}\n",
    "l(\\beta) & = \\log(L(\\beta)) = \\log(\\prod_{i = 1}^{n} \\pi_i^{y_i}(1-\\pi_i)^{1 - y_i}) = \\sum_{i = 1}^n y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i) \\\\& = \\sum_{i = 1}^n y_i \\log(\\dfrac{\\pi_i}{1-\\pi_i}) + \\log(1-\\pi_i) \\\\ & = \\sum_{i = 1}^{n} [y_i \\log(\\dfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}) + (1-y_i)\\log(\\dfrac{1}{1+e^{\\beta^Tx_i}}] \\\\& = \\sum_{i = 1}^n y_i(\\beta^Tx_i) - \\log(1 + e^{\\beta^Tx_i}) \n",
    "\\end{align*}\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-assault",
   "metadata": {},
   "source": [
    "## Problem (b) : *Logistic Regression*\n",
    "\n",
    "### *(1) The gradient*\n",
    "The parameters estimates are obtained by maximizing the above log-likelihood function. $\\\\$\n",
    "The first-order partial derivatives of the log-likelihood for each $\\beta_k, k = 0, 1, 2, ...,p$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial l(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n y_i x_{ik} - \\pi_i x_{ik} = \\sum_{i = 1}^n x_{ik}(y_i - \\pi_i)\n",
    "\\end{align*}\n",
    "\n",
    "### *(2) Run the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "neutral-incident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient:  [-6.95662675  5.06982462  1.74274771 -0.69012498  3.87815305  2.09302448\n",
      "  0.11135059  3.04400915  1.50906571 -0.4922035  -2.72527485 -1.09253232\n",
      " -1.41684284  0.97737043  2.09496576 -0.044454    0.68156447  2.62873262\n",
      " -1.78590736 -0.90757517  0.57433208 -3.14078097  1.99495681  1.91200894\n",
      "  1.69982922  0.38438736 -3.74208969 -1.19479945  1.71105622  2.82480776\n",
      " -0.05220857 -0.46149617 -0.41142094 -3.39828793] \n",
      "\n",
      "Error Num: 12   Accuracy: 0.8309859154929577 \n",
      "\n",
      "Confusion Matrix| True c(0) True nc(1) Total\n",
      "  Predict c(0)  | 16         1         17\n",
      "  Predict nc(1) | 11        43         54\n",
      "  Total         | 27        44         71\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "def GradientD(x, y, stepsize, iters):\n",
    "    beta = np.random.random_sample((x.shape[1],1))\n",
    "    for i in range(iters):\n",
    "        yy = pi_val(np.dot(x, beta))\n",
    "        grad = np.dot(x.T, y - yy)\n",
    "        beta = beta + stepsize*grad\n",
    "    return beta\n",
    "\n",
    "def LR(x, y, stepsize, iters):\n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    beta = GradientD(x, y, stepsize, iters)\n",
    "    return beta\n",
    "\n",
    "def OutputFuncLo(beta, res):\n",
    "    print('Coefficient: ', beta.flatten(), '\\n')\n",
    "    print('Error Num:', res['error'], '  Accuracy:', res['accu'], '\\n')\n",
    "\n",
    "betaLo = LR(xtrain, fytrain, 0.01, 500)\n",
    "resLo = PredictFunc(xtest, fytest, betaLo)\n",
    "OutputFuncLo(betaLo, resLo)\n",
    "confusionM(resLo['truey'], resLo['prey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-avatar",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-point",
   "metadata": {},
   "source": [
    "## Problem (c) : *Ridge Regression*\n",
    "### (1) *Loss Function* \n",
    "\\begin{align*}\n",
    "J(\\beta) = \\lambda \\sum_{k = 1}^p \\beta_k^2  - \\sum_{i = 1}^n [y_i \\beta^T x_i  - \\log(1 + e^{\\beta^T x_i})]\\end{align*}\n",
    "\n",
    "### (2) *Explain why we use Ridge regression*\n",
    "\n",
    "### (3) *Run the model*\n",
    "\n",
    " \\begin{align*}\\text{Gradient of Ridge Regression}:\\dfrac{\\partial J(\\beta)}{\\partial \\beta_k} = 2\\lambda \\beta_k -\\sum_{i=1}^n x_{ik}(y_i-\\pi_i)\\end{align*}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "sunset-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient:  [-3.344032    1.73033749  1.43237088  0.29798663  2.28564127  1.10387447\n",
      "  0.20617683  1.8295677   1.02511576 -0.03762162 -0.93761814 -0.49178095\n",
      " -0.65697438  1.01476359  0.84972688  0.19799413  0.04467586  1.03398558\n",
      " -0.54037014 -0.3179909   0.24764916 -1.9588307   1.37560491  0.70357886\n",
      "  0.77024357  0.78512616 -2.32129601 -0.59899016  0.78072296  0.9055665\n",
      "  0.18767598 -0.43685696 -0.01692763 -1.72000486] \n",
      "\n",
      "Error Number:  12  Accuracy:  0.8309859154929577  Best Lambda:  0.03 \n",
      "\n",
      "Confusion Matrix| True c(0) True nc(1) Total\n",
      "  Predict c(0)  | 16         1         17\n",
      "  Predict nc(1) | 11        43         54\n",
      "  Total         | 27        44         71\n"
     ]
    }
   ],
   "source": [
    "def GradientD3(x, y, stepsize, iters, lam3):\n",
    "    beta = 0.5*np.ones((x.shape[1],1))\n",
    "    #beta = np.random.random_sample((x.shape[1],1)) # 34*1\n",
    "    for i in range(iters):\n",
    "        yy = pi_val(np.dot(x, beta))\n",
    "        grad = -np.dot(x.T, y - yy)+ 2*lam3 * beta\n",
    "        beta = beta - stepsize*grad\n",
    "    return beta\n",
    "\n",
    "def RR(x, y, stepsize, iters, lam3):\n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    beta = GradientD3(x, y, stepsize, iters, lam3)\n",
    "    return beta\n",
    "\n",
    "def cvkfold(data, lamrange, k):\n",
    "    folds = np.array_split(data, k)\n",
    "    tempmse = np.zeros((len(lamrange), 1))\n",
    "    for i in range(k):\n",
    "        train = folds.copy()\n",
    "        test = folds[i]\n",
    "        del train[i]\n",
    "        \n",
    "        train = pandas.concat(train, sort=False)\n",
    "        #print(test.shape)\n",
    "        xtrain_train = train.iloc[:, :33]\n",
    "        #print(xtrain_train)\n",
    "        ytrain_train = train.iloc[:, 33:]\n",
    "        xtrain_test = test.iloc[:, :33]\n",
    "        ytrain_test = test.iloc[:, 33:]\n",
    "        \n",
    "        for lam3 in lamrange:\n",
    "            betatemp = RR(xtrain_train, ytrain_train, 0.01, 100, lam3)\n",
    "            restemp = PredictFunc(xtrain_test, ytrain_test, betatemp)\n",
    "            tempmse[lamrange.index(lam3)] += restemp['mse']\n",
    "    return tempmse/k # average mse\n",
    "\n",
    "    \n",
    "def usebestlam(lambdalist, lambdapotential, xtrain, fytrain, xtest, fytest):\n",
    "    up = 100\n",
    "    index = 0\n",
    "    for i in range(len(lambdalist)):\n",
    "        if lambdalist[i] < up:\n",
    "            up = lambdalist[i]\n",
    "            index = i\n",
    "    bestlam = lambdapotential[index]\n",
    "    beta1 = RR(xtrain, fytrain, 0.01, 100, bestlam)\n",
    "    res = PredictFunc(xtest, fytest, beta1)\n",
    "    print('Coefficient: ', beta1.flatten(), '\\n')\n",
    "    print('Error Number: ', res['error'], ' Accuracy: ', res['accu'], ' Best Lambda: ', bestlam, '\\n')\n",
    "    confusionM(res['truey'], res['prey'])\n",
    "    return res['accu'], bestlam\n",
    "\n",
    "lambdapotential = [0.03, 0.01, 0.02, 0.04, 0.05, 0.06, 0.07]\n",
    "data = pandas.concat([xtrain, fytrain.reindex(xtrain.index)], axis=1)\n",
    "lambdalist = cvkfold(data, lambdapotential, 5)\n",
    "#print(lambdalist)\n",
    "acc, lam = usebestlam(lambdalist, lambdapotential, xtrain, fytrain, xtest, fytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-furniture",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-respondent",
   "metadata": {},
   "source": [
    "## Problem (d) : *LASSO Regression*\n",
    "### (1) *Loss Function* \n",
    "\\begin{align*}\n",
    "L(\\beta) = \\lambda \\sum_{j=1}^n\\left| \\beta_j \\right| - \\sum_{i = 1}^n [y_i \\beta^T x_i  - \\log(1 + e^{\\beta^T x_i})]  \\end{align*}\n",
    "\n",
    "### (2) *Explain why we use Lasso regression*\n",
    "\n",
    "### (3) *Run the model*\n",
    "\n",
    " \\begin{align*}\\text{Gradient of Lasso Regression}:\\dfrac{\\partial L(\\beta)}{\\partial \\beta_k} = S_{\\lambda t}(\\beta) -\\sum_{i=1}^p x_{ik}(y_i-\\pi_i)\\end{align*}\n",
    " \n",
    " \\begin{align*}\\text{Soft-Thresholding Function}: [S_{\\lambda t}(\\beta)] = \\left\\{\\begin{matrix}\n",
    "\\beta - t\\lambda & \\beta > t\\lambda\\\\ \n",
    "0 & -t\\lambda \\le \\beta \\le t \\lambda\\\\\n",
    "\\beta + t\\lambda & \\beta < -t\\lambda  \n",
    "\\end{matrix}\\right.\\end{align*}\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "substantial-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient:  [-2.56195010e+00  1.26246152e+00  1.11629823e+00  1.31392984e-01\n",
      "  1.60138044e+00  7.16622663e-01  2.63230690e-01  1.36635774e+00\n",
      "  5.71142500e-01  8.33839419e-04 -4.97083904e-01 -9.40214097e-02\n",
      " -3.28440440e-01  5.85059578e-01  3.82038926e-01  1.19003500e-01\n",
      " -2.50278938e-02  6.13503424e-01 -2.11457182e-01 -1.91839584e-03\n",
      "  1.79844038e-02 -1.37618011e+00  9.35823680e-01  3.45359389e-01\n",
      "  4.55240942e-01  3.60374627e-01 -1.61807132e+00 -2.97429587e-01\n",
      "  4.52473156e-01  4.56870211e-01  1.20887707e-01 -1.89647706e-01\n",
      "  1.65811295e-03 -1.10822119e+00] \n",
      "\n",
      "Accuracy:  0.8591549295774648  Best Lambda:  0.004\n",
      "Confusion Matrix| True c(0) True nc(1) Total\n",
      "  Predict c(0)  | 18         1         19\n",
      "  Predict nc(1) | 9        43         52\n",
      "  Total         | 27        44         71\n"
     ]
    }
   ],
   "source": [
    "def soft_threshold(beta, lam):\n",
    "    t = 1\n",
    "    for i in range(len(beta)):\n",
    "        if beta[i] > t*lam:\n",
    "            beta[i] -= t*lam\n",
    "        elif beta[i] < -t*lam:\n",
    "            beta[i] += t*lam\n",
    "        else:\n",
    "            beta[i] = 0\n",
    "    return beta\n",
    "\n",
    "def GradientD4(x, y, stepsize, iters, lam3):\n",
    "    beta = 0.5*np.ones((x.shape[1],1)) \n",
    "    #beta = np.random.random_sample((x.shape[1],1)) # 34*1\n",
    "    for i in range(iters):\n",
    "        yy = pi_val(np.dot(x, beta))\n",
    "        grad = -np.dot(x.T, y - yy)+ soft_threshold(beta, lam3)\n",
    "        beta = beta - (stepsize*grad)\n",
    "    return beta\n",
    "\n",
    "def LA(x, y, stepsize, iters, lam3):\n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    beta = GradientD4(x, y, stepsize, iters, lam3)\n",
    "    return beta\n",
    "\n",
    "def cvkfold4(data, lamrange, k):\n",
    "    folds = np.array_split(data, k)\n",
    "    tempmse = np.zeros((len(lamrange), 1))\n",
    "    for i in range(k):\n",
    "        train = folds.copy()\n",
    "        test = folds[i]\n",
    "        del train[i]\n",
    "        \n",
    "        train = pandas.concat(train, sort=False)\n",
    "        #print(test.shape)\n",
    "        xtrain_train = train.iloc[:, :33]\n",
    "        #print(xtrain_train)\n",
    "        ytrain_train = train.iloc[:, 33:]\n",
    "        xtrain_test = test.iloc[:, :33]\n",
    "        ytrain_test = test.iloc[:, 33:]\n",
    "        \n",
    "        for lam3 in lamrange:\n",
    "            betatemp = LA(xtrain_train, ytrain_train, 0.01, 100, lam3)\n",
    "            restemp = PredictFunc(xtrain_test, ytrain_test, betatemp)\n",
    "            tempmse[lamrange.index(lam3)] += restemp['mse']\n",
    "    return tempmse/k # average mse\n",
    "\n",
    "def usebestlam4(lambdalist, lambdapotential, xtrain, fytrain, xtest, fytest):\n",
    "    up = 100\n",
    "    index = 0\n",
    "    for i in range(len(lambdalist)):\n",
    "        if lambdalist[i] < up:\n",
    "            up = lambdalist[i]\n",
    "            index = i\n",
    "    bestlam = lambdapotential[index]\n",
    "    beta1 = LA(xtrain, fytrain, 0.01, 300, bestlam)\n",
    "    res = PredictFunc(xtest, fytest, beta1)\n",
    "    print('Coefficient: ', beta1.flatten(), '\\n')\n",
    "    print('Accuracy: ', res['accu'], ' Best Lambda: ', bestlam)\n",
    "    confusionM(res['truey'], res['prey'])\n",
    "    return res['accu'], bestlam\n",
    "\n",
    "lambdapotential = [0.005, 0.004, 0.003, 0.0033, 0.0027]\n",
    "data = pandas.concat([xtrain, fytrain.reindex(xtrain.index)], axis=1)\n",
    "lambdalist = cvkfold4(data, lambdapotential, 5)\n",
    "acc, lam = usebestlam4(lambdalist, lambdapotential, xtrain, fytrain, xtest, fytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-animation",
   "metadata": {},
   "source": [
    "## Problem (e) : *Best Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-revision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
